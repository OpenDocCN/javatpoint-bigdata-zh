# RDD 行动

> 原文：<https://www.javatpoint.com/apache-spark-rdd-operations>

RDD 提供两种类型的操作:

*   转换
*   行动

## 转换

在 Spark 中，转换的作用是从现有数据集创建新数据集。转换被认为是懒惰的，因为它们只在一个动作需要一个结果返回给驱动程序时才计算。

让我们看看一些常用的 RDD 变换。

| 转换 | 描述 |
| 地图(功能) | 它返回一个新的分布式数据集，该数据集是通过函数 func 传递源的每个元素而形成的。 |
| 过滤器(功能) | 它返回一个新的数据集，该数据集是通过选择 func 返回 true 的源元素而形成的。 |
| 平面地图(功能) | 这里，每个输入项可以映射到零个或多个输出项，因此 func 应该返回一个序列，而不是一个单独的项。 |
| 映射分区(func) | 它类似于 map，但在 RDD 的每个分区(块)上分别运行，因此在 t 类型的 RDD 上运行时，func 必须是 Iterator 类型 <t>=> Iterator</t> |
| mappartitionswithinindex(func) | 它类似于 mapPartitions，为 func 提供一个代表分区索引的整数值，因此在 t 型 RDD 上运行时，func 的类型必须是(Int，Iterator <t>) => Iterator</t> |
| 样本(有替换、部分、种子) | 它使用给定的随机数生成器种子对数据的分数部分进行采样，有或没有替换。 |
| 联合(其他数据集) | 它返回一个新的数据集，该数据集包含源数据集中的元素和参数的并集。 |
| 交集(其他数据集) | 它返回一个新的 RDD，其中包含源数据集中元素和参数的交集。 |
| 不同的([数字签名]) | 它返回一个包含源数据集不同元素的新数据集。 |
| group by key([numpatitions]) | 当在(K，V)对的数据集上调用时，它返回(K，Iterable <v>)对的数据集。</v> |
| reduceByKey(func)(数字分区) | 当在(K，V)对的数据集上调用时，返回(K，V)对的数据集，其中使用给定的 reduce 函数 func 聚合每个键的值，该函数的类型必须是(V，V) => V |
| aggregateByKey(zero value)(seqOp，combOp，[numpatitions]) | 当在(K，V)对的数据集上调用时，返回(K，U)对的数据集，其中每个键的值使用给定的组合函数和中性“零”值进行聚合。 |
| sortByKey([升序]，[numpatitions]) | 它返回一个键值对数据集，按照布尔升序参数中指定的升序或降序按键排序。 |
| 联接(其他数据集、[数字关联]) | 当在(K，V)和(K，W)类型的数据集上调用时，返回(K，(V，W))对的数据集，其中包含每个键的所有元素对。通过左外部连接、右外部连接和完全外部连接支持外部连接。 |
| cogroup(otherDataset，[numpatitions]) | 当在(K，V)和(K，W)类型的数据集上调用时，返回(K，(可迭代<v>，可迭代<w>)元组的数据集。这个操作也被称为 groupWith。</w>T3】</v> |
| 笛卡尔坐标(其他数据集) | 当在类型为 T 和 U 的数据集上调用时，返回(T，U)对(所有元素对)的数据集。 |
| 管道(命令，[envVars]) | 通过一个 shell 命令，比如一个 Perl 或 bash 脚本，管道化 RDD 的每个分区。 |
| 联合(numPartitions) | 它将 RDD 中的分区数量减少到了个。 |
| 分割区(numPartitions) | 它会随机重组 RDD 的数据，以创建更多或更少的分区，并在这些分区之间进行平衡。 |
| ortWithinPartitions(分割区) | 它根据给定的分区器对 RDD 进行重新分区，并在每个结果分区内，根据记录的关键字对记录进行排序。 |

## 行动

在 Spark 中，操作的作用是在数据集上运行计算后向驱动程序返回值。

让我们看看一些常用的 RDD 动作。

| 行动 | 描述 |
| 减少(函数) | 它使用函数 func(接受两个参数并返回一个参数)聚合数据集的元素。该函数应该是可交换的和关联的，以便能够正确地并行计算。 |
| 收集() | 它在驱动程序中将数据集的所有元素作为数组返回。这通常在返回足够小的数据子集的过滤器或其他操作之后有用。 |
| 计数() | 它返回数据集中的元素数量。 |
| 第一次() | 它返回数据集的第一个元素(类似于 take(1))。 |
| 拿(n) | 它返回一个数组，其中包含数据集的第一个 n lemon。 |
| 提取样本(带替换，编号，[种子]) | 它返回一个数组，其中包含数据集 num 元素的随机样本，有或没有替换，可选地预先指定一个随机数生成器种子。 |
| 外卖(n，[订购]) | 它使用自然顺序或自定义比较器返回 RDD 的前 n 个元素。 |
| saveAsTextFile(路径) | 它用于将数据集的元素作为文本文件(或文本文件集)写入本地文件系统、HDFS 或任何其他 Hadoop 支持的文件系统中的给定目录。Spark 在每个元素上调用 toString，将其转换为文件中的一行文本。 |
| saveAsSequenceFile(路径)
(Java 和 Scala) | 它用于将数据集的元素作为 Hadoop 序列文件写入本地文件系统、HDFS 或任何其他 Hadoop 支持的文件系统中的给定路径。 |
| saveAsObjectFile(路径)
(Java 和 Scala) | 它用于使用 Java 序列化以简单的格式编写数据集的元素，然后可以使用 SparkContext.objectFile()加载这些元素。 |
| countByKey() | 它仅在类型(K，V)的 rdd 上可用。因此，它返回一个包含每个键的计数的(K，Int)对的散列表。 |
| foreach(函数) | 它在数据集的每个元素上运行一个函数函数来处理副作用，例如更新累加器或与外部存储系统交互。 |

* * *