# 什么是 ETL？

> 原文：<https://www.javatpoint.com/what-is-etl-in-ms-azure>

“提取、转换和加载”是“提取、转换和加载”的缩写

在数据集成技术中，ETL 过程至关重要。企业可以使用 ETL 从各种来源收集数据，并将其组合到一个单一的集中位置。不同种类的数据也可以在 ETL 的帮助下一起操作。

## 概观

一个 [ETL](https://www.javatpoint.com/what-is-etl) 过程收集和提炼许多类型的数据，然后将其传送到[数据仓库](https://www.javatpoint.com/data-warehouse)，如红移、 [Azure](https://www.javatpoint.com/microsoft-azure) 或大查询。

还可以使用 ETL 在各种源、目的地和分析工具之间迁移数据。因此，ETL 过程对于生成业务智能和执行更大的数据管理计划至关重要。

## ETL 的功能

ETL 由三个基本步骤组成，它们是提取、转换和加载。通过执行这 3 个步骤，我们提取数据，并在处理后将其存储到特定的目的地。

### 第一步:提取

只有一小部分企业依赖单一的数据类型或技术。为了开发业务信息，大多数组织从一系列来源管理数据，并采用各种数据分析方法。必须允许数据在系统和应用程序之间自由移动，这样复杂的数据策略才能发挥作用。

在将数据重新定位到新位置之前，必须首先从其源中检索数据。在 ETL 过程的第一阶段，结构化和非结构化数据被导入并合并到单个存储位置或数据库中。数据的提取可以从各种来源进行，通过保持它们的原始形式-

*   已经存在的数据库和遗留系统
*   云环境、混合环境和内部环境
*   在销售和营销中的应用
*   应用和移动设备
*   数据仓库
*   数据存储平台
*   分析工具
*   客户关系管理系统

虽然可以手动提取数据，但是很耗时，容易出错。ETL 工具使提取过程自动化，从而产生更加可靠和高效的工作流。

### 第二步:转型

在这一步中，我们还可以进行数据验证。这将确保根据需要转换的所有数据都是正确的。在这个 ETL 过程中，我们保持数据的质量。我们还可以利用法规来帮助我们的组织履行其报告义务。数据转换过程中有各种子过程:

*   清理-检查数据的差异和缺失值。
*   标准化-数据集是根据格式化规则格式化的。
*   重复数据消除-删除或销毁重复数据。
*   验证-异常被识别，不可用的数据被丢弃。
*   排序-信息按类型分类。
*   其他任务-为了提高数据质量，可以使用任何附加/可选的规则。

转换被广泛认为是 ETL 过程中最关键的一步。数据转换提高了数据的完整性，并确保数据在到达新位置时完全符合要求并随时可用。

### 步骤 3:加载

现在在这一步中，新转换的数据被加载到一个新的目的地，作为 ETL 过程的最后一步。数据可以批量加载(满载)或以预定的时间间隔加载(增量加载)。

**完全加载** -在 ETL 完全加载场景中，从转换装配线出来的所有东西都会进入数据仓库中新的、唯一的条目。尽管这对于研究目的有时可能是有利的，但完全加载会导致数据集呈指数级增长，变得难以管理。

**增量加载** -增量加载过程是一种不太全面但更可控的技术。增量加载将新数据与现有数据进行比较，只有在发现新的唯一数据时才会创建新记录。这种体系结构使商业智能能够由更小、更便宜的数据仓库来维护和管理。

## ETL 和商业智能

公司现在可以从比以往更多的来源获得更多的数据，这使得数据策略比以往任何时候都更加复杂。ETL 允许将大量数据转化为有用的商业智能。

考虑公司可以访问的数据量。除了由设施中的传感器和装配线上的机器收集的数据之外，该组织还收集营销、销售、物流和财务数据。

为了进行分析，所有这些数据都必须被移除、更改并加载到一个新的位置。在这种情况下，我们可以使用 ETL 方法，通过以下方式充分利用我们的数据

### 提供单一观点

多个数据集需要时间和协调，这会导致效率低下和延迟。使用 ETL，数据库和不同类型的数据被组合成一个单一的、内聚的视图。

### 提供历史背景

ETL 帮助公司将旧平台和应用程序的数据与新平台和应用程序的数据合并。这创建了一个长期的数据图，允许将较旧的数据集与较新的数据集进行比较。

### 提高效率和生产率

使用 ETL 软件，手工编码的数据迁移变得更加容易。因此，开发人员和他们的团队可能会更多地关注创新，而不是创建数据移动和格式化代码这种耗时的杂务。

## 为数据迁移构建正确的 ETL 策略

有两种不同的方法可以让我们轻松完成 ETL 过程。不管怎样，两者都有各自的要求和成本。在各种情况下，企业可以将自己 ETL 的开发委托给开发人员。然而，这一程序耗时，容易延误，而且费用高昂。

大多数公司现在使用一个 ETL 工具，以便平稳地完成数据集成过程。ETL 工具以其速度、可伸缩性和成本效益以及与大型数据管理技术集成的能力而闻名。ETL 工具还附带了各种数据质量和治理功能。

我们还需要决定开源产品是否适合我们公司，因为它们通常提供更多的自由，并帮助用户避免供应商锁定。

Talend 数据结构是一组连接我们所有数据的应用程序，无论其来源或目的地如何。

数据在一个标准的地方被收集、清理和处理。最后，数据被放入数据存储中，并从那里进行查询。导入数据由遗留的 ETL 处理，ETL 会在将数据存储到关系数据引擎之前对其进行清理。

Azure HDInsight 支持各种 Apache [Hadoop](https://www.javatpoint.com/hadoop-tutorial) 环境组件，用于大规模的 ETL。

接下来的部分将讨论每个 ETL 步骤及其组件。

### 管弦乐编曲

在 ETL 管道的每个阶段都使用编排。在 HDInsight 中，ETL 过程经常需要同时使用几个独立的产品。考虑以下场景:

*   我们可以用 Apache Hive 清理一部分数据，用 Apache Pig 清理另一部分数据。
*   我们可以使用 Azure 数据工厂将数据从 Azure 数据湖存储加载到 Azure SQL 数据库中。

需要业务流程才能在正确的时间运行正确的作业。这是非常重要的一部分，必须加以注意。

### Apache Oozie

Apache Oozie 是一个 Hadoop 任务管理工作流协调框架。Oozie 是一个 Hadoop 集成，在 HDInsight 集群中运行。

### 蔚蓝数据工厂

在平台即服务的伪装下，Azure 数据工厂提供编排功能(PaaS)。Azure 数据工厂是一个在云中运行的数据集成解决方案。它帮助我们使用数据驱动的工作流来协调和自动化数据传输和转换。

### 输入文件和输出文件的存储

大部分源数据文件直接上传到 Azure Storage 或 Azure Data Lake Storage，这是最简单的方法之一。通常，文件是平面格式，例如 CSV 或 Azure 中支持的任何其他格式，尽管它支持各种各样的数据格式。

### 蔚蓝存储

已经为 Azure 存储设定了具体的适应目标。当处理大量微小文件时，Azure Storage 最适合大多数分析节点。Azure Storage 提供相同的速度，无论文件大小如何，只要它在我们的帐户限制范围内。可以以一致的性能存储万亿字节的数据。

为了存储网络日志或传感器数据，我们可以轻松地使用 blob 存储。斑点存储器有多种形状和尺寸，我们可以根据需要选择。

如果我们想横向扩展对许多 blobs 的访问，它们可以很容易地分散在多个服务器中。另一方面，单个 blob 由单个服务器提供服务。

对于 blob 存储，Azure 存储包括一个 WebHDFS API 层。对于任何类型的清理过程或需要从该存储中获取数据的任何其他过程，HDInsight 都可以访问所有文件。这与这些服务(HDFS)使用 Hadoop 分布式文件系统的方式类似。

### ADLS 或蔚蓝数据湖存储

蔚蓝数据湖存储也简称为 ADLS。这是一个托管的超大规模分析数据存储，用于存储数据。它与 [HDFS](https://www.javatpoint.com/hdfs) 兼容，并遵循类似的架构方法。数据湖存储在总容量和文件大小方面具有无限的灵活性。

[Azure 数据工厂](https://www.javatpoint.com/azure-data-factory)通常用于将数据送入数据湖存储。数据湖存储 SDK、AdlCopy 服务、Apache DistCp 和 Apache Sqoop 也是可选的。我们选择的服务取决于数据的位置。

通过 Azure 事件中心或 Apache Storm 进行的事件摄取适用于数据湖存储。

### 蔚蓝突触分析

为了保存准备好的结果，Azure Synapse Analytics 是一个不错的选择。

Azure Synapse Analytics 是一个专门为分析工作负载设计的关系数据库存储。它使用分区表来扩展。可以使用多个节点来拆分表。我们需要根据创建 Azure Synapse Analytics 时的需求来选择节点。它们可以随后扩展，但这是一个活跃的过程，可能需要数据移动。

### 阿帕契巴塞

在微软 Azure 中，我们有一个特殊的功能，即 Apache HBas，它是 Azure HDInsight 中的一个关键价值存储。这是一个基于 Hadoop 并受谷歌大表启发的免费开源 NoSQL 数据库。对于海量的非结构化和半结构化数据， [HBase](https://www.javatpoint.com/hbase) 支持高性能随机访问和强大的一致性。

在使用 HBase 之前，我们不必定义列或数据类型，因为它是一个无模式数据库。数据按列族组织并存储在表的行中。

为了处理千兆字节的数据，开源代码在数千个节点上线性扩展。

HBase 依赖 Hadoop 环境中的分布式应用程序来提供数据冗余、批处理和其他功能。

对于未来的分析，糖化血红蛋白酶是保存传感器和日志数据的一个有用的地方。

### Azure SQL 数据库

Azure 中有三种 PaaS 关系数据库:

*   Azure SQL 数据库是一个微软的 SQL Server 实现。
*   Oracle MySQL 是在 MySQL 的 Azure 数据库中实现的。
*   用于 PostgreSQL 的 Microsoft Azure 数据库是一个 PostgreSQL 实现。

要扩展这些项目，请添加更多的 CPU 和内存。高级磁盘也可以与这些项目一起使用，以提高输入/输出性能。

### Apache Sqoop

Apache Sqoop 是一种跨结构化、半结构化和非结构化数据源快速高效地移动数据的工具。

Sqoop 使用 MapReduce 导入和导出数据，这允许并行处理和容错。

### 阿帕奇水槽

Apache Flume 是一种用于快速收集、聚合和传输大量分布式、可靠且可用的日志数据的服务。它的适应性架构是建立在流式数据流的概念上的。Flume 具有可配置的可靠性方法，使其具有鲁棒性和容错性。它提供了许多故障转移和恢复功能。

Flume 采用了一个简单的可扩展数据模型，支持在线分析应用。

Azure HDInsight 不支持 Apache Flume。为了从内部 Hadoop 安装将数据传输到 Azure Blob 存储或 Azure 数据湖存储，可以使用水槽轻松完成任务。

### 改变

数据存储在所需位置后，必须对其进行清理、合并或为特定的使用模式做好准备。

* * *