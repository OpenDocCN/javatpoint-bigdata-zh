# PySpark UDF

> 哎哎哎:# t0]https://www . javatppoint . com/pyspark-UDF

Spark SQL 提供了 PySpark UDF(用户定义函数)，用于定义一个新的基于列的函数。它扩展了 Spark SQL 的用于转换数据集的 DSL 的词汇。

### 将功能注册为 UDF

```

def cube(s):
    return s*s*s
spark.udf.register("cubewithPython", cube)

```

我们可以选择设置 UDF 的返回类型。默认返回类型为**字符串类型。**考虑以下示例:

```

from pyspark.sql.types import LongType
def cube_typed(s):
  return s * s
spark.udf.register("cubewithPython", cube_typed, LongType()) 

```

### 调用 UDF 函数

```

spark.range(1, 20).registerTempTable("test")

```

PySpark UDF 的功能与熊猫**地图()**功能和 **apply()** 功能相同。这些功能用于熊猫系列和数据框。在下面的例子中，我们将创建一个 PySpark 数据帧。

```

import pandas as pd
# We consider the following example data
df_pd = pd.DataFrame(
    data={'integers': [1, 2, 3],
     'floats': [-1.0, 0.5, 2.7],
     'integer_arrays': [[1, 2], [3, 4, 5], [6, 7, 8, 9]]}
)
df = spark.createDataFrame(df_pd)
df.printSchema() # It will print the Schema
df.show() # Display the Dataframe

```

代码将打印数据框的模式和数据框。

**输出**

```
root
 |-- integers: long (nullable = true)
 |-- floats: double (nullable = true)
 |-- integer_arrays: array (nullable = true)
 |    |-- element: long (containsNull = true)

+--------+------+--------------+
|integers|floats|integer_arrays|
+--------+------+--------------+
|       1|  -1.0|        [1, 2]|
|       2|   0.5|     [3, 4, 5]|
|       3|   2.7|  [6, 7, 8, 9]|
+--------+------+--------------+

```

### 评估顺序和空值检查

PySpark SQL 不能保证子表达式的求值顺序保持不变。不必从左到右或以任何其他固定顺序评估运算符或函数的 Python 输入。例如，逻辑**和**或**表达式没有从左到右的“短路”语义。**

 **因此，依赖布尔表达式的求值顺序是非常不安全的。例如 **WHERE** 和 **HAVING** 子句的顺序，因为这样的表达式和子句可以在查询优化和计划期间重新排序。如果一个 UDF 依赖于 SQL 中的短路语义(求值顺序)来进行空值检查，那么不能保证空值检查会在调用 UDF 之前发生。

```

spark.udf.register("strlen_nullsafe", lambda s: len(s) if not s is None else -1, "int")
spark.sql("select s from test1 where s is not null and strlen_nullsafe(s) > 1") // ok
spark.sql("select s from test1 where if(s is not null, strlen(s), null) > 1")   // ok

```

### 基本类型输出

让我们考虑一个平方一个数字的函数 **square()** ，并将这个函数注册为 Spark UDF。

```

def square(x):
   return x**2

```

现在我们把它转换成 UDF。注册时，我们必须使用 **pyspark.sql.types.** 指定数据类型。spark UDF 的问题是它不能将整数转换为浮点，而 Python 函数对整数值和浮点值都有效。如果输入数据类型与输出数据类型不匹配，PySpark UDF 将返回一列空值。让我们考虑以下程序:

```

from pyspark.sql.types import IntegerType
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
square_udf_int = F.udf(lambda z: square(z), IntegerType())
(
    df.select('integers',
              'floats',
              square_udf_int('integers').alias('int_squared'),
              square_udf_int('floats').alias('float_squared'))
    .show()
)

```

**输出:**

```
+--------+------+-----------+-------------+
|integers|floats|int_squared|float_squared|
+--------+------+-----------+-------------+
|       1|  -1.0|          1|         null|
|       2|   0.5|          4|         null|
|       3|   2.7|          9|         null|
+--------+------+-----------+-------------+

```

我们可以看到上面的输出，对于浮点输入，它返回 null。现在看另一个例子。

### 用浮点型输出向 UDF 注册

```

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
from pyspark.sql.types import FloatType
square_udf_float = F.udf(lambda z: square(z), FloatType())
(
    df.select('integers',
              'floats',
              square_udf_float('integers').alias('int_squared'),
              square_udf_float('floats').alias('float_squared'))
    .show()
)

```

**输出:**

```
+--------+------+-----------+-------------+
|integers|floats|int_squared|float_squared|
+--------+------+-----------+-------------+
|       1|  -1.0|       null|          1.0|
|       2|   0.5|       null|         0.25|
|       3|   2.7|       null|         7.29|
+--------+------+-----------+-------------+

```

### 使用 Python 函数指定浮点类型输出

这里，对于整数输入，我们也强制输出为浮点。

```

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
def square_float(x):
    return float(x**2)
square_udf_float2 = F.udf(lambda z: square_float(z), FloatType())
(
    df.select('integers',
              'floats',
              square_udf_float2('integers').alias('int_squared'),
              square_udf_float2('floats').alias('float_squared'))
    .show()
)

```

**输出:**

```
+--------+------+-----------+-------------+
|integers|floats|int_squared|float_squared|
+--------+------+-----------+-------------+
|       1|  -1.0|        1.0|          1.0|
|       2|   0.5|        4.0|         0.25|
|       3|   2.7|        9.0|         7.29|
+--------+------+-----------+-------------+

```

### 复合类型输出

如果 Python 函数的输出是列表形式，那么输入值必须是列表，注册 UDF 时用 **ArrayType()** 指定。考虑以下代码:

```

from pyspark.sql.types import ArrayType
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
def square_list(x):
    return [float(val)**2 for val in x]
square_list_udf = F.udf(lambda y: square_list(y), ArrayType(FloatType()))
df.select('integer_arrays', square_list_udf('integer_arrays')).show()

```

**输出:**

```
+--------------+------------------------+
|integer_arrays|(integer_arrays)|
+--------------+------------------------+
|        [1, 2]|              [1.0, 4.0]|
|     [3, 4, 5]|       [9.0, 16.0, 25.0]|
|  [6, 7, 8, 9]|    [36.0, 49.0, 64.0...|
+--------------+------------------------+ 
```

### 一些常见的 UDF 问题

*   py 4 javaerror

这是与 UDF 合作时最常见的例外。它来自 Python 和 Spark 之间不匹配的数据类型。如果 Python 函数使用了来自 Python 模块的数据类型，如 **numpy.ndarray，**，那么 UDF 抛出一个异常。

```

import numpy as np
# Example data
d_np = pd.DataFrame({'int_arrays': [[1,2,3], [4,5,6]]})
df_np = spark.createDataFrame(d_np)
df_np.show()

```

**输出:**

```
+----------+
|int_arrays|
+----------+
| [1, 2, 3]|
| [4, 5, 6]|
+----------+

```

在下面的例子中，我们正在创建一个返回 nd.ndarray 的函数。它们的值也是 Numpy 对象 Numpy.int32，而不是 Python 原语。

```

def square_array_wrong(x):
    return np.square(x)
square_array_wrong([1,2,3])

```

**输出:**

```
array([1, 4, 9], dtype=int32)

```

如果我们执行下面的代码，它会抛出一个异常 **Py4JavaError。**

```

from pyspark.sql.types import ArrayType
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
spark_square_array_wrong = F.udf(square_array_wrong, ArrayType(FloatType()))
df_np.withColumn('doubled', spark_square_array_wrong('int_arrays')).show()

```

**输出:**

![PySpark UDF](../Images/75d657c0c6bd6dd4fb5b7f0976d02ed2.png)

这种异常的解决方案是将其转换回一个值为 Python 原语的列表。

```

from pyspark.sql.types import ArrayType
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import udf
def square_array_right(x):
    return np.square(x).tolist()

spark_square_array_right = F.udf(square_array_right, ArrayType(IntegerType()))
zz = df_np.withColumn('squared', spark_square_array_right('int_arrays'))
zz.show()

```

**输出:**

```
+----------+------------+
|int_arrays|     squared|
+----------+------------+
| [1, 2, 3]|   [1, 4, 9]|
| [4, 5, 6]|[16, 25, 36]|
+----------+------------+

```

在上面的代码中，我们描述了异常的解决方案。现在自己做，观察两个程序的区别。

*   **缓慢**

PySpark 还有一个缺点；与 Python 的对应版本相比，它需要大量的运行时间。就文件大小而言，数据量小是速度慢的原因之一。Spark 将整个数据帧发送给一个也是唯一一个执行器，并让其他执行器等待。解决方案是对数据帧进行重新分区。例如:

```

df_repartitioned  = df.repartitioned(100) 

```

当我们对数据进行重新分区时，每个执行器一次处理一个分区，从而减少了执行时间。

* * ***