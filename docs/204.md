# 斯巴基 Conf

> 哎哎哎:# t0]https://www . javatppoint . com/pyspark-spark conf

**什么是 SparkConf？**

**Spark配置**为任何Spark应用提供配置。要在本地集群或数据集上启动任何 Spark 应用程序，我们需要设置一些配置和参数，这可以使用 **SparkConf 来完成。**

**Sparkconf 的特性及其用法**

使用 PySpark 时，Sparkconf 最常用的功能如下:

*   **设置(键、值)-**
*   **setMastervalue(值)-**
*   箭头名称(值)
*   **获取(键，默认值=无)-**
*   **设置Spark家园(值)-**

考虑下面的例子来理解 SparkConf 的一些属性:

```
from pyspark.conf import SparkConf
from pyspark.context import SparkContext
conf = SparkConf().setAppName('PySpark Demo App').setMaster('local[2]')
conf.get('spark.master')
conf.get('spark.app.name')

```

**输出:**

```
'PySpark Demo App'

```

任何 spark 程序做的第一件事就是创建一个 SparkContext 对象，告诉应用程序如何访问集群。为了完成任务，您需要实现 SparkConf，以便 SparkContext 对象包含关于应用程序的配置信息。下面我们将详细描述 SparkContext:

### sparks context(储蓄上下文)

**什么是 SparkContext？**

**SparkContext** 是我们运行任何 Spark 应用程序时启动的第一个也是最基本的东西。任何 Spark 驱动程序应用程序最重要的一步是生成 SparkContext。它是任何Spark衍生应用或功能的入口。默认情况下，它在 Pyspark 中作为 **sc** 提供。

#### 注意:您需要记住，创建另一个变量而不是 sc 会产生错误。

### 参数:

SparkContext 接受下面描述的参数:

**大师**

集群的网址连接到Spark。

**应用程序名称**

任务的名称。

**Spark家园**

SparkHome 是一个 Spark 安装目录。

**pyFiles**

。zip 或。py 文件被发送到集群，然后被添加到 PYTHONPATH。

**环境**

它表示工作节点环境变量。

**批次大小**

Python 对象的编号代表批处理大小。如果要禁用批处理，请将其设置为 1。它根据对象大小 0 自动选择批处理大小，对于无限批处理大小设置 1。

**串行器**

它表示序列化程序，一个 RDD。

conf

它设置了所有的Spark属性。那里有一个{SparkConf}的对象。

profiler _ cls

这是一类自定义概要文件，用于进行概要分析，不过要确保**py spark . profiler . basicprofiler**是默认的。

主参数和应用程序名是参数中使用最广泛的参数。以下是任何 **PySpark** 应用的初始代码。

* * *