# PySpark 广播和累加器

> 原文：<https://www.javatpoint.com/pyspark-broadcast-and-accumulator>

Apache Spark 使用共享变量进行并行处理。并行处理在更短的时间内完成一项任务。当驱动程序向集群上的执行器发送任务时，共享变量的副本会被传输到集群的每个节点，以便可以使用它来执行任务。

Apache Spark 支持以下类型的共享变量。

*   **广播**
*   **蓄能器**

### 1.广播

广播变量是共享变量之一，用于保存所有节点上的数据副本。它允许程序员在每台机器上缓存一个只读变量，而不是将它的一个副本与任务一起发送。例如，以高效的方式向每个节点提供大型输入数据集的副本。下面的代码提供了 PySpark 的广播类的细节。

```

class pyspark.Broadcast (
   sc = None, 
   value = None, 
   pickle_registry = None, 
   path = None
)

```

上面的代码显示了 Broadcast 变量的用法。它有一个名为**值的属性。**存储数据，用于返回广播值。

```

from pyspark import SparkContext 
sc = SparkContext("local", "Broadcast app") 
words_new = sc.broadcast(["scala", "java", "hadoop", "spark", "akka"]) 
data = words_new.value 
print("Stored data -> %s" % (data)) 
elem = words_new.value[2] 
print("Printing a particular element in RDD " %s (elem))

```

它将给出以下输出:

**输出:**

```
Stored data -> ['scala', 'java', 'hadoop', 'spark', 'akka']
Printing a particular element in RDD -> hadoop

```

### 2.累加器

累加器变量用于通过关联和交换操作组合信息。我们可以对**求和运算**和**计数器**使用累加(在 MapReduce 中)。以下代码详细描述了累加器:

```

class pyspark.Accumulator(aid, value, accum_param)

```

我们考虑下面的例子来描述如何使用累加器变量。它具有与广播变量相同的属性值；该属性还存储数据。然后它返回累加器值，但只能在驱动程序中使用。

在下面的示例中，累加器变量由多个节点使用，并返回一个累加值。

```

from pyspark import SparkContext 
sc = SparkContext("local", "Accumulator app") 
num = sc.accumulator(10) 
def f(x): 
   global num 
   num+=x 
rdd = sc.parallelize([20,30,40,50]) 
rdd.fosreach(f) 
final = num.value 
print("Accumulated value is -> %i" % (final))

```

上述程序保存为累加器. py，并给出以下输出:

**输出:**

```
The accumulate value is 15

```

* * *