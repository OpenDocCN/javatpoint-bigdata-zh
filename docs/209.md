# PySpark 迷你文件

> 原文：<https://www.javatpoint.com/pyspark-sparkfiles>

PySpark 提供了使用 **sc.addFile.** 上传文件的功能，我们还可以使用 **SparkFiles.get.** 获取工作目录的路径，此外，为了解析通过 **SparkContext.addFile()，**添加的文件的路径，SparkFiles 中有以下类型的类方法，例如:

*   **获取(文件名)**
*   **getrotdirectory()**

#### 注意:迷你文件只包含可以根据需要使用的类方法。用户不应创建迷你文件实例。

让我们详细了解一下上课方法。

## PySpark 迷你文件的类方法

*   **获取(文件名)**

**get(文件名)**指定通过 **SparkContext.addFile()添加的文件的路径。**

```
import os
class SparkFiles(object):
     """
     Resolves paths to files added through
     L{SparkContext.addFile()<pyspark.context.SparkContext.addFile>}.
     SparkFiles consist of only class methods; users should not create SparkFiles
     instances.
     """
      root_directory = None
      is_running_on_worker = False
      sc = None
      def __init__(self):
         raise NotImplementedError("Do not construct SparkFiles objects")
     @classmethod
       def get(cls, filename):
         """
         Get the absolute path of a file added through C{SparkContext.addFile()}.
         """
         path = os.path.join(SparkFiles.getRootDirectory(), filename)
         return os.path.abspath(path)
        @classmethod
        def getRootDirectory(cls):

```

*   **getrotdirectory()**

此类方法指定根目录的路径。基本上包含了通过 **SparkContext.addFile()添加的整个文件。**

```
import os
  class SparkFiles(object):
     """
     Resolves paths to files added through
     L{SparkContext.addFile()<pyspark.context.SparkContext.addFile>}.
     SparkFiles contains only classmethods; users should not create SparkFiles
     instances.
     """
      root_directory = None
      is_running_on_worker = False
      sc = None
      def __init__(self):
         raise NotImplementedError("Do not construct SparkFiles objects")
     @classmethod
      def get(cls, filename):
        ...
     @classmethod
      def getRootDirectory(cls):
         """
         Than Get the root directory which contains files added through
         C{SparkContext.addFile()}.
         """
 if cls._is_running_on_worker:
     return cls._root_directory
 else:
     # This will have to change if we support multiple SparkContexts:
     return cls._sc._jvm.spark.SparkFiles.getRootDirectory()

```

* * *