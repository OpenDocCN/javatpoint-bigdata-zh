# PySpark 状态追踪器(jtracker)

> 原文:[https://www.javatpoint.com/pyspark-statustracker](https://www.javatpoint.com/pyspark-statustracker)

PySpark 提供了低级状态报告 API，用于监控作业和阶段进度。我们可以使用这些 API 跟踪作业。这些 API 有意提供非常弱的兼容性语义，因此这些 API 的用户在处理自由/缺失信息时应该小心。

状态跟踪程序可能知道作业阶段的 id，但不会知道这些阶段的信息。在这种情况下，PySpark 提供了 **getStageInfo** ，对于有效的阶段 id，它不返回任何内容。

APIS**spark . ui . retained stages**和 **spark.ui.retainedjobs** 将提供当前工作和阶段的信息。

```

def __init__(self, jtracker):
self._jtracker = jtracker

```

*   **get active job ids()**

它返回一个数组，该数组由所有活动作业的 id 组成。语法如下:

```

def getActivateJobsIds(self):
	return sorted((list(self.jtracker.getActivateJobs())))

```

*   **getActiveStageIds()**

它返回一个数组，该数组由所有活动阶段的 id 组成。语法如下:

```

def getActiveStageIds(self):
	return sorted(list(self.jtracker.getActiveStageIds)

```

*   **getjobidsforggroup(jobGroup =无)**

此功能用于获取单个职务组中所有已知的职务。这些工作以列表的形式出现。如果作业组为“无”，则它将返回与作业组无关的所有类型的作业。

它返回可能处于运行、失败和完成状态的作业，但顺序可能会有所不同。考虑下面的代码。

```

def getJobIdsForGroup(self, jobGroup=None):
	return list(self._jtracker.getJobIdsForGroup(jobGroup))

```

*   **getJobInfo（jobId）**

有时，由于垃圾收集，上述函数无法获取作业信息，对于这些情况，我们使用 getJobInfo。它返回一个 SparkJobInfo 对象或无。

```

def getJobinfo(self,jobId):
	job = self.jtracker.getJobInfo(jobId)
	if job is not None:
	return SparkJobInfo (jobId,job.stageIds())

```

*   **getStageInfo(stageId)**

类似地，有时由于垃圾收集无法找到作业信息，那么就使用 getStageinfo(stageId)。它返回一个 **SparkJobInfo** 对象，或者 **None。**

```

def getStageInfo(self, stageId):
       stage1 = self._jtracker.getStageInfo()	
       if(stage is not None):
       # Extract them in batch for best performance
       attrs = [getattr(stage,f) () for f in SparkStageInfo._fields[1:]]
       return SparkStageInfo(stageId,*attrs) 

```

* * *