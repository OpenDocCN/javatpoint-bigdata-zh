# PySpark 存储级别

> 原文：<https://www.javatpoint.com/pyspark-storagelevel>

PySpark StorageLevel 用于决定 RDD 应该如何存储在内存中。它还确定天气序列化 RDD 和天气复制 RDD 分区。在 Apache Spark 中，它负责 RDD 应该保存在内存中还是应该存储在磁盘上，或者两者都保存。它包含常用的 PySpark **StorageLevels，**像 MEMORY_ONLY 这样的静态常量。

以下代码块由存储级别的类定义组成-

```

class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)

```

### 类变量

有不同的 PySpark **存储级别**来决定 RDD 的存储，例如:

*   **DISK_ONLY:** 存储级别(真、假、假、假、1)
*   **DISK_ONLY_2:** 存储级别(真、假、假、假、2)
*   **内存和磁盘:**存储级别(真、真、假、假、1)
*   **MEMORY _ AND _ DISK _ 2:**storage level(真、真、假、假、2)
*   **MEMORY _ AND _ DISK _ SER:**storage level(真、真、假、假、1)
*   **MEMORY _ AND _ DISK _ SER _ 2:**storage level(真、真、假、假、2)
*   **MEMORY_ONLY:** 存储级别(假、真、假、假、1)
*   **MEMORY_ONLY_2:** 存储级别(假、真、假、假、2)
*   **MEMORY_ONLY_SER:** 存储级别(假、真、假、假、1)
*   **MEMORY_ONLY_SER_2:** 存储级别(假、真、假、假、2)
*   **OFF_HEAP:** 存储级别(真、真、真、假、1)

### 实例方法

```

__init__(self, useDisk, useMemory, deserialized, replication=1)

```

## PySpark 存储级别示例

这里我们使用存储级别 **Memory_And_Disk_2，**这意味着 RDD 分区将具有 2 的复制。

```

from pyspark import SparkContext
import pyspark
sc = SparkContext (
  "local",
  "StorageLevel app"
)
rdd1 = sc.parallelize([1,2])
rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )
rdd1.getStorageLevel()
print(rdd1.getStorageLevel())

```

**输出:**

```
Disk Memory Serialized 2x Replicated

```

* * *