# Hadoop 安装

> 原文:[https://www.javatpoint.com/hadoop-installation](https://www.javatpoint.com/hadoop-installation)

**Hadoop 所需的环境:**Hadoop 的生产环境是 UNIX，但也可以在使用 Cygwin 的 Windows 中使用。运行地图缩减程序需要 Java 1.6 或更高版本。对于在 UNIX 环境中从 tar ball 安装 Hadoop，您需要

1.  Java 安装
2.  SSH 安装
3.  Hadoop 安装和文件配置

## 1) Java 安装

**第一步。**在提示符下输入“java -version”，查看是否安装了 java。如果没有，那么从 http://www . Oracle . com/tech network/java/javase/downloads/JDK 7-downloads-1880260 . html 下载 Java。焦油 filejdk-7u71-linux-x64.tar.gz 将被下载到你的系统。

**第二步。**使用以下命令提取文件

```
#tar zxf jdk-7u71-linux-x64.tar.gz

```

**第三步。**要让所有 UNIX 用户都可以使用 java，请将文件移动到/usr/local 并设置路径。在提示符下切换到 root 用户，然后键入以下命令将 jdk 移动到/usr/lib。

```
# mv jdk1.7.0_71 /usr/lib/

```

现在在~/。bashrc 文件添加以下命令来设置路径。

```
# export JAVA_HOME=/usr/lib/jdk1.7.0_71
# export PATH=PATH:$JAVA_HOME/bin

```

现在，您可以通过在提示符下键入“java -version”来检查安装。

## 2) SSH 安装

SSH 用于与主计算机和从计算机交互，无需任何密码提示。首先，在主系统和从系统上创建一个 Hadoop 用户

```
# useradd hadoop
# passwd Hadoop

```

要映射节点，请打开所有计算机上/etc/文件夹中的主机文件，并将 ip 地址与其主机名放在一起。

```
# vi /etc/hosts

```

输入下面的行

```
190.12.1.114    hadoop-master
190.12.1.121    hadoop-salve-one
190.12.1.143   hadoop-slave-two

```

在每个节点中设置 SSH 密钥，这样它们就可以在没有密码的情况下相互通信。相同的命令有:

```
# su hadoop 
$ ssh-keygen -t rsa 
$ ssh-copy-id -i ~/.ssh/id_rsa.pub tutorialspoint@hadoop-master 
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp1@hadoop-slave-1 
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp2@hadoop-slave-2 
$ chmod 0600 ~/.ssh/authorized_keys 
$ exit

```

## 3) Hadoop 安装

Hadoop 可以从 http://developer.yahoo.com/hadoop/tutorial/module3.html 下载

现在提取 Hadoop 并将其复制到一个位置。

```
$ mkdir /usr/hadoop
$ sudo tar vxzf  hadoop-2.2.0.tar.gz ?c /usr/hadoop

```

更改 Hadoop 文件夹的所有权

```
$sudo chown -R hadoop  usr/hadoop

```

更改 Hadoop 配置文件:

所有文件都存在于/usr/local/Hadoop/etc/hadoop 中

1)在 hadoop-env.sh 文件中添加

```
export JAVA_HOME=/usr/lib/jvm/jdk/jdk1.7.0_71

```

2)在核心站点. xml 中，在配置选项卡之间添加以下内容，

```
 <property><name>fs.default.name</name>
<value>hdfs://hadoop-master:9000</value></property> 
 <property><name>dfs.permissions</name>
<value>false</value></property> 

```

3)在配置选项卡之间的 hdfs-site.xmladd 跟随中，

```
 <property><name>dfs.data.dir</name>
<value>usr/hadoop/dfs/name/data</value>
<final>true</final></property> 
 <property><name>dfs.name.dir</name>
<value>usr/hadoop/dfs/name</value>
<final>true</final></property> 
 <property><name>dfs.replication</name>
<value>1</value></property> 

```

4)打开 Mapred-site.xml 并进行如下所示的更改

```
 <property><name>mapred.job.tracker</name>
<value>hadoop-master:9001</value></property> 

```

5)最后，更新您的$HOME/。bahsrc

```
cd $HOME
vi .bashrc
Append following lines in the end and save and exit
#Hadoop variables 
export JAVA_HOME=/usr/lib/jvm/jdk/jdk1.7.0_71
export HADOOP_INSTALL=/usr/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin 
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL 
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL 
export YARN_HOME=$HADOOP_INSTALL

```

在从属计算机上，使用以下命令安装 Hadoop

```
# su hadoop 
$ cd /opt/hadoop 
$ scp -r hadoop hadoop-slave-one:/usr/hadoop 
$ scp -r hadoop hadoop-slave-two:/usr/Hadoop

```

配置主节点和从节点

```
$ vi etc/hadoop/masters
hadoop-master

$ vi etc/hadoop/slaves
hadoop-slave-one 
hadoop-slave-two

```

在此格式之后，命名节点并启动所有守护程序

```
# su hadoop 
$ cd /usr/hadoop 
$ bin/hadoop namenode -format

$ cd $HADOOP_HOME/sbin
$ start-all.sh

```

最简单的步骤是使用 cloudera，因为它附带了所有预装的东西，可以从 http://content . uda city-data . com/courses/ud 617/cloud era-uda city-Training-VM-4 . 1 . 1 . c . zip 下载